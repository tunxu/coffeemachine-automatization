{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6acb9164",
   "metadata": {},
   "source": [
    "# Keyword recognition with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f6dd246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba14abce",
   "metadata": {},
   "source": [
    "Load the Mini Speech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c362e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_set_to_16000(tensor):\n",
    "    if tensor.size()[0] < 16000:\n",
    "        tensor = tensor[:160000, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75266408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, sample_rate=16000, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.transform = transform\n",
    "\n",
    "        # Collect all file paths and labels\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        self.label_names = sorted(os.listdir(root_dir))\n",
    "\n",
    "        for label_idx, label_name in enumerate(self.label_names):\n",
    "            label_folder = os.path.join(root_dir, label_name)\n",
    "            for file_name in os.listdir(label_folder):\n",
    "                if file_name.endswith(\".wav\"):\n",
    "                    self.file_paths.append(os.path.join(label_folder, file_name))\n",
    "                    self.labels.append(label_idx)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "\n",
    "        # Resample if needed\n",
    "        if sr != self.sample_rate:\n",
    "            waveform =  F.resample(\n",
    "            waveform,\n",
    "            sr,\n",
    "            16000,\n",
    "            lowpass_filter_width=64,\n",
    "            rolloff=0.9475937167399596,\n",
    "            resampling_method=\"sinc_interp_kaiser\",\n",
    "            beta=14.769656459379492,\n",
    "        )\n",
    "\n",
    "        # Convert to mono\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "        # Apply transforms (e.g. MelSpectrogram)\n",
    "        if self.transform:\n",
    "            features = self.transform(waveform)\n",
    "        else:\n",
    "            features = waveform\n",
    "\n",
    "        # Pad shorter clips, trim longer clips\n",
    "        max_len = self.sample_rate  # 1 second = 16000 samples\n",
    "        if waveform.shape[1] < max_len:\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, max_len - waveform.shape[1]))\n",
    "        else:\n",
    "            waveform = waveform[:, :max_len]\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a577669",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.MelSpectrogram(sample_rate=16000, n_mels=64)\n",
    "\n",
    "train_dataset = MyAudioDataset(\"data2\", sample_rate=16000, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe44b03",
   "metadata": {},
   "source": [
    "## CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00467462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "821ed99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, 1, n_mels, time]\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = x.mean(dim=[2, 3])  # [batch, channels]\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4209eab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeywordCNN(num_classes=len(train_dataset.label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a8bc92",
   "metadata": {},
   "source": [
    "Minimal training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9c6b050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.5121\n",
      "Epoch 2, Loss: 1.4762\n",
      "Epoch 3, Loss: 1.4347\n",
      "Epoch 4, Loss: 1.4367\n",
      "Epoch 5, Loss: 1.3767\n",
      "Epoch 6, Loss: 1.3973\n",
      "Epoch 7, Loss: 1.3157\n",
      "Epoch 8, Loss: 1.3332\n",
      "Epoch 9, Loss: 1.3420\n",
      "Epoch 10, Loss: 1.2718\n",
      "Epoch 11, Loss: 1.1757\n",
      "Epoch 12, Loss: 1.1853\n",
      "Epoch 13, Loss: 1.1572\n",
      "Epoch 14, Loss: 1.1414\n",
      "Epoch 15, Loss: 1.1037\n",
      "Epoch 16, Loss: 1.1328\n",
      "Epoch 17, Loss: 1.0616\n",
      "Epoch 18, Loss: 1.0081\n",
      "Epoch 19, Loss: 1.0036\n",
      "Epoch 20, Loss: 0.9243\n",
      "Epoch 21, Loss: 0.9305\n",
      "Epoch 22, Loss: 0.9930\n",
      "Epoch 23, Loss: 0.9642\n",
      "Epoch 24, Loss: 0.8379\n",
      "Epoch 25, Loss: 0.8548\n",
      "Epoch 26, Loss: 0.8016\n",
      "Epoch 27, Loss: 0.8089\n",
      "Epoch 28, Loss: 0.7047\n",
      "Epoch 29, Loss: 0.8391\n",
      "Epoch 30, Loss: 0.6684\n",
      "Epoch 31, Loss: 0.6936\n",
      "Epoch 32, Loss: 0.6587\n",
      "Epoch 33, Loss: 0.6647\n",
      "Epoch 34, Loss: 0.6442\n",
      "Epoch 35, Loss: 0.5361\n",
      "Epoch 36, Loss: 0.5299\n",
      "Epoch 37, Loss: 0.6270\n",
      "Epoch 38, Loss: 0.5524\n",
      "Epoch 39, Loss: 0.5485\n",
      "Epoch 40, Loss: 0.5475\n",
      "Epoch 41, Loss: 0.4583\n",
      "Epoch 42, Loss: 0.5065\n",
      "Epoch 43, Loss: 0.4385\n",
      "Epoch 44, Loss: 0.4366\n",
      "Epoch 45, Loss: 0.3665\n",
      "Epoch 46, Loss: 0.3566\n",
      "Epoch 47, Loss: 0.4536\n",
      "Epoch 48, Loss: 0.3844\n",
      "Epoch 49, Loss: 0.3125\n",
      "Epoch 50, Loss: 0.3171\n",
      "Epoch 51, Loss: 0.2557\n",
      "Epoch 52, Loss: 0.2599\n",
      "Epoch 53, Loss: 0.2164\n",
      "Epoch 54, Loss: 0.1984\n",
      "Epoch 55, Loss: 0.2838\n",
      "Epoch 56, Loss: 0.2831\n",
      "Epoch 57, Loss: 0.2385\n",
      "Epoch 58, Loss: 0.1877\n",
      "Epoch 59, Loss: 0.2997\n",
      "Epoch 60, Loss: 0.2347\n",
      "Epoch 61, Loss: 0.2548\n",
      "Epoch 62, Loss: 0.2496\n",
      "Epoch 63, Loss: 0.1822\n",
      "Epoch 64, Loss: 0.1867\n",
      "Epoch 65, Loss: 0.2297\n",
      "Epoch 66, Loss: 0.1653\n",
      "Epoch 67, Loss: 0.1896\n",
      "Epoch 68, Loss: 0.1460\n",
      "Epoch 69, Loss: 0.1286\n",
      "Epoch 70, Loss: 0.1906\n",
      "Epoch 71, Loss: 0.2076\n",
      "Epoch 72, Loss: 0.1397\n",
      "Epoch 73, Loss: 0.1852\n",
      "Epoch 74, Loss: 0.1035\n",
      "Epoch 75, Loss: 0.0798\n",
      "Epoch 76, Loss: 0.0712\n",
      "Epoch 77, Loss: 0.0962\n",
      "Epoch 78, Loss: 0.1135\n",
      "Epoch 79, Loss: 0.1904\n",
      "Epoch 80, Loss: 0.1231\n",
      "Epoch 81, Loss: 0.1249\n",
      "Epoch 82, Loss: 0.0750\n",
      "Epoch 83, Loss: 0.0605\n",
      "Epoch 84, Loss: 0.1084\n",
      "Epoch 85, Loss: 0.2304\n",
      "Epoch 86, Loss: 0.2481\n",
      "Epoch 87, Loss: 0.1816\n",
      "Epoch 88, Loss: 0.1093\n",
      "Epoch 89, Loss: 0.0959\n",
      "Epoch 90, Loss: 0.1379\n",
      "Epoch 91, Loss: 0.1438\n",
      "Epoch 92, Loss: 0.1357\n",
      "Epoch 93, Loss: 0.1105\n",
      "Epoch 94, Loss: 0.0815\n",
      "Epoch 95, Loss: 0.0836\n",
      "Epoch 96, Loss: 0.1866\n",
      "Epoch 97, Loss: 0.1763\n",
      "Epoch 98, Loss: 0.0972\n",
      "Epoch 99, Loss: 0.0586\n",
      "Epoch 100, Loss: 0.0661\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e06bd556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model to PATH\n",
    "PATH = 'model/kws_model.pth'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06422915",
   "metadata": {},
   "source": [
    "Testing on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c54471b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_file(model, filepath, transform, sample_rate, label_names, device):\n",
    "    # Load audio\n",
    "    waveform, sr = torchaudio.load(filepath)\n",
    "    \n",
    "    # Resample if needed\n",
    "    if sr != sample_rate:\n",
    "        waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=sample_rate)(waveform)\n",
    "    \n",
    "    # Convert to mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    # Pad or trim to 1 second (for 16kHz = 16000 samples)\n",
    "    max_len = sample_rate\n",
    "    if waveform.shape[1] < max_len:\n",
    "        waveform = F.pad(waveform, (0, max_len - waveform.shape[1]))\n",
    "    else:\n",
    "        waveform = waveform[:, :max_len]\n",
    "    \n",
    "    # Apply transform (e.g. MelSpectrogram)\n",
    "    features = transform(waveform).unsqueeze(0).to(device)  # [1, 1, n_mels, time]\n",
    "    \n",
    "    # Run model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(features)\n",
    "        pred_idx = torch.argmax(outputs, dim=1).item()\n",
    "    \n",
    "    pred_label = label_names[pred_idx]\n",
    "    print(f\"ðŸŽ™ï¸ Predicted keyword: {pred_label}\")\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb2752f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ™ï¸ Predicted keyword: fernando\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fernando'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example configuration\n",
    "sample_rate = 16000  # or whatever your model was trained on\n",
    "\n",
    "# Assuming you already defined your transform, e.g.:\n",
    "# transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=64)\n",
    "\n",
    "# And you have a trained model + label names\n",
    "file_path = \"data2/ok/ok_hasan.wav\"\n",
    "\n",
    "predict_file(model, file_path, transform, sample_rate, train_dataset.label_names, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
