{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4b9e986",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "# Set the seed value for experiment reproducibility.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19067a32",
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "\n",
    "Downloading a smaller version of the mini speech command data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0099d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'data/mini_speech_commands_extracted/mini_speech_commands'\n",
    "\n",
    "data_dir = pathlib.Path(DATASET_PATH)\n",
    "if not data_dir.exists():\n",
    "  tf.keras.utils.get_file(\n",
    "      'mini_speech_commands.zip',\n",
    "      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
    "      extract=True,\n",
    "      cache_dir='.', cache_subdir='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb7d07",
   "metadata": {},
   "source": [
    "The dataset is divided into 8 folders corresponding to ```no```, ```yes```, ```go```, ```down```, ```up```, ```left```, ```right```, ```stop```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b4dd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = np.array(tf.io.gfile.listdir(\"data/mini_speech_commands_extracted/mini_speech_commands\"))\n",
    "commands = commands[(commands != 'README.md') & (commands != '.DS_Store')]\n",
    "print('Commands:', commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9a9fff",
   "metadata": {},
   "source": [
    "By using ```tf.keras.utils.audio_dataset_from_directory``` we can split the dataset into training and validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de3b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory=\"data/mini_speech_commands_extracted/mini_speech_commands\",\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    seed=0,\n",
    "    output_sequence_length=16000, # padding shorter files to one second, trimming longer files to one second\n",
    "    subset='both')\n",
    "\n",
    "label_names = np.array(train_ds.class_names)\n",
    "print()\n",
    "print(\"label names:\", label_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f0fe23",
   "metadata": {},
   "source": [
    "The dataset now contains batches of audio clips and integer labels. The audio clips have a shape of (batch, samples, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f45db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b1d5d9",
   "metadata": {},
   "source": [
    "As the files are only mono channeled we need to drop the extra axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28407ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze(audio, labels):\n",
    "  audio = tf.squeeze(audio, axis=-1)\n",
    "  return audio, labels\n",
    "\n",
    "train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a669d",
   "metadata": {},
   "source": [
    "The ```utils.audio_dataset_from_directory``` function only returns up to two splits. It's a good idea to keep a test set separate from your validation set. Ideally you'd keep it in a separate directory, but in this case you can use ```Dataset.shard``` to split the validation set into two halves. Note that iterating over any shard will load all the data, and only keep its fraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff147ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = val_ds.shard(num_shards=2, index=0)\n",
    "val_ds = val_ds.shard(num_shards=2, index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebcd6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_audio, example_labels in train_ds.take(1):  \n",
    "  print(example_audio.shape)\n",
    "  print(example_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c95ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names[[1,1,3,0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de7fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "rows = 3\n",
    "cols = 3\n",
    "n = rows * cols\n",
    "for i in range(n):\n",
    "  plt.subplot(rows, cols, i+1)\n",
    "  audio_signal = example_audio[i]\n",
    "  plt.plot(audio_signal)\n",
    "  plt.title(label_names[example_labels[i]])\n",
    "  plt.yticks(np.arange(-1.2, 1.2, 0.2))\n",
    "  plt.ylim([-1.1, 1.1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd14c1f",
   "metadata": {},
   "source": [
    "## Converting wavefroms into spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51147392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(waveform):\n",
    "  # Convert the waveform to a spectrogram via a STFT.\n",
    "  spectrogram = tf.signal.stft(\n",
    "      waveform, frame_length=255, frame_step=128)\n",
    "  # Obtain the magnitude of the STFT.\n",
    "  spectrogram = tf.abs(spectrogram)\n",
    "  # Add a `channels` dimension, so that the spectrogram can be used\n",
    "  # as image-like input data with convolution layers (which expect\n",
    "  # shape (`batch_size`, `height`, `width`, `channels`).\n",
    "  spectrogram = spectrogram[..., tf.newaxis]\n",
    "  return spectrogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a95bb3",
   "metadata": {},
   "source": [
    "Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd749bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "  label = label_names[example_labels[i]]\n",
    "  waveform = example_audio[i]\n",
    "  spectrogram = get_spectrogram(waveform)\n",
    "\n",
    "  print('Label:', label)\n",
    "  print('Waveform shape:', waveform.shape)\n",
    "  print('Spectrogram shape:', spectrogram.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cf5327",
   "metadata": {},
   "source": [
    "## Visualize the spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f705a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spectrogram, ax):\n",
    "  if len(spectrogram.shape) > 2:\n",
    "    assert len(spectrogram.shape) == 3\n",
    "    spectrogram = np.squeeze(spectrogram, axis=-1)\n",
    "  # Convert the frequencies to log scale and transpose, so that the time is\n",
    "  # represented on the x-axis (columns).\n",
    "  # Add an epsilon to avoid taking a log of zero.\n",
    "  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
    "  height = log_spec.shape[0]\n",
    "  width = log_spec.shape[1]\n",
    "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
    "  Y = range(height)\n",
    "  ax.pcolormesh(X, Y, log_spec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3aa318",
   "metadata": {},
   "source": [
    "Plot the example's waveform over time and the corresponding spectrogram (frequencies over time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7420451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
    "timescale = np.arange(waveform.shape[0])\n",
    "axes[0].plot(timescale, waveform.numpy())\n",
    "axes[0].set_title('Waveform')\n",
    "axes[0].set_xlim([0, 16000])\n",
    "\n",
    "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
    "axes[1].set_title('Spectrogram')\n",
    "plt.suptitle(label.title())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87685a2f",
   "metadata": {},
   "source": [
    "Create Spectogram Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a65fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spec_ds(ds):\n",
    "  return ds.map(\n",
    "      map_func=lambda audio,label: (get_spectrogram(audio), label),\n",
    "      num_parallel_calls=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8463f011",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spectrogram_ds = make_spec_ds(train_ds)\n",
    "val_spectrogram_ds = make_spec_ds(val_ds)\n",
    "test_spectrogram_ds = make_spec_ds(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e811634c",
   "metadata": {},
   "source": [
    "Examine the spectrograms for different examples of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b12f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf3c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 3\n",
    "n = rows*cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n",
    "\n",
    "for i in range(n):\n",
    "    r = i // cols\n",
    "    c = i % cols\n",
    "    ax = axes[r][c]\n",
    "    plot_spectrogram(example_spectrograms[i].numpy(), ax)\n",
    "    ax.set_title(label_names[example_spect_labels[i].numpy()])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdfe19a",
   "metadata": {},
   "source": [
    "## Building and Training the model\n",
    "\n",
    "By using ```Dataset.cache``` and ```Dataset.prefetch``` read latency during training can be reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab7ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_spectrogram_ds = train_spectrogram_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
    "#val_spectrogram_ds = val_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "#test_spectrogram_ds = test_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb51be2",
   "metadata": {},
   "source": [
    "The model will be a simple CNN. \n",
    "Our tf.keras.Sequential model will use the following Keras preprocessing layers:\n",
    "- ```tf.keras.layers.Resizing```: to downsample the input to enable the model to train faster.\n",
    "- ```tf.keras.layers.Normalization```: to normalize each pixel in the image based on its mean and standard deviation.\n",
    "\n",
    "For the ```Normalization``` layer, its ```adapt``` method needs to be called upon the training data first \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af635330",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = example_spectrograms.shape[1:]\n",
    "print('Input shape:', input_shape)\n",
    "num_labels = len(label_names)\n",
    "\n",
    "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
    "norm_layer = layers.Normalization()\n",
    "# Fit the state of the layer to the spectrograms\n",
    "# with `Normalization.adapt`.\n",
    "norm_layer.adapt(data=train_spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    # Downsample the input.\n",
    "    layers.Resizing(32, 32),\n",
    "    # Normalize.\n",
    "    norm_layer,\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_labels),\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ffa334",
   "metadata": {},
   "source": [
    "Configure the Keras model with the Adam optimizer and the cross-entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee18f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d764bcfa",
   "metadata": {},
   "source": [
    "Training the model over 10 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29075274",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "history = model.fit(\n",
    "    train_spectrogram_ds,\n",
    "    validation_data=val_spectrogram_ds,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=len(train_ds) // 128,\n",
    "    validation_steps=len(val_ds) // 128,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(verbose=1, patience=2)],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b442d",
   "metadata": {},
   "source": [
    "Plotting validation loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a8a8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = history.history\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss [CrossEntropy]')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.epoch, 100*np.array(metrics['accuracy']), 100*np.array(metrics['val_accuracy']))\n",
    "plt.legend(['accuracy', 'val_accuracy'])\n",
    "plt.ylim([0, 100])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy [%]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2b6419",
   "metadata": {},
   "source": [
    "Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c058742",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_spectrogram_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dce36f",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff766b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_spectrogram_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65061fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.argmax(y_pred, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90fa008",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.concat(list(test_spectrogram_ds.map(lambda s,lab: lab)), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd7e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx,\n",
    "            xticklabels=label_names,\n",
    "            yticklabels=label_names,\n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f4432b",
   "metadata": {},
   "source": [
    "Running inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f67c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_dir/'no/01bb6a2a_nohash_0.wav'\n",
    "x = tf.io.read_file(str(x))\n",
    "x, sample_rate = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\n",
    "x = tf.squeeze(x, axis=-1)\n",
    "waveform = x\n",
    "x = get_spectrogram(x)\n",
    "x = x[tf.newaxis,...]\n",
    "\n",
    "prediction = model(x)\n",
    "x_labels = ['no', 'yes', 'down', 'go', 'left', 'up', 'right', 'stop']\n",
    "plt.bar(x_labels, tf.nn.softmax(prediction[0]))\n",
    "plt.title('No')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
